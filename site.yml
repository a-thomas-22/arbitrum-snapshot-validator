---
- name: Import EC2 provisioning playbook
  import_playbook: provision_ec2.yml

- name: Setup Arbitrum Node
  hosts: all
  become: true
  remote_user: ubuntu
  gather_facts: false

  vars:
    ansible_ssh_private_key_file: "{{ lookup('env', 'SSH_PRIVATE_KEY') | default(lookup('env', 'PRIVATE_KEY_PATH')) }}"
    base_data_path: /data
    arbitrum_path: "{{ base_data_path }}/arbitrum"
    nitro_path: "{{ arbitrum_path }}/nitro"
    nitro_logs_path: "{{ arbitrum_path }}/logs"
    ansible_ssh_common_args: "-o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=/dev/null"

  pre_tasks:
    - name: Debug connection information
      debug:
        msg:
          - "SSH User: {{ ansible_user | default('ubuntu') }}"
          - "SSH Key File: {{ ansible_ssh_private_key_file }}"
          - "SSH Host: {{ ansible_host | default('unknown') }}"
      delegate_to: localhost
      run_once: true

  handlers:
    - name: restart docker
      service:
        name: docker
        state: restarted

  tasks:
    - name: Validate chain name
      fail:
        msg: "Invalid chain_name '{{ chain_name }}'. Valid options are: {{ chain_id_map.keys()|join(', ') }}"
      when: chain_name not in chain_id_map

    - name: Set chain ID
      set_fact:
        chain_id: "{{ chain_id_map[chain_name] }}"

    - name: Debug - Show chain ID
      debug:
        msg: "Chain ID for {{ chain_name }}: {{ chain_id }}"

    - name: Update apt cache
      apt:
        update_cache: yes
        cache_valid_time: 3600

    - name: Install required packages
      apt:
        name:
          - apt-transport-https
          - ca-certificates
          - curl
          - software-properties-common
          - aria2
          - xfsprogs
          - mdadm
          - nvme-cli
        state: present

    - name: Check existing RAID configuration
      command: mdadm --detail /dev/md0
      register: raid_check
      failed_when: false
      changed_when: false

    - name: Set RAID status
      set_fact:
        raid_configured: "{{ raid_check.rc == 0 }}"

    - name: Get NVMe instance store devices
      shell: nvme list | awk '/Instance Storage/ {print $1}'
      register: instance_stores
      changed_when: false

    - name: Debug - Show detected instance stores
      debug:
        msg: "Detected instance stores: {{ instance_stores.stdout_lines }}"

    - name: Fail if no instance store devices found
      fail:
        msg: "No instance store devices found. Are you running on an i3en instance?"
      when: instance_stores.stdout_lines|length == 0

    - name: Create RAID0 array
      when: not raid_configured
      command: >
        mdadm --create --verbose --level=0 /dev/md0 
        --name=DATA 
        --raid-devices={{ instance_stores.stdout_lines | length }} 
        {{ instance_stores.stdout_lines | join(' ') }}
      args:
        creates: /dev/md0

    - name: Wait for device
      when: not raid_configured
      wait_for:
        path: /dev/md0
        timeout: 30

    - name: Check if RAID array has filesystem
      when: not raid_configured
      command: file -s /dev/md0
      register: raid_fs_check
      changed_when: false

    - name: Create XFS filesystem
      command: mkfs.xfs -f /dev/md0
      when: not raid_configured

    - name: Create base mount point
      file:
        path: "{{ base_data_path }}"
        state: directory
        mode: "0755"
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"

    - name: Get RAID array UUID
      shell: blkid -s UUID -o value /dev/md0 || true
      register: raid_uuid
      changed_when: false

    - name: Check if already mounted
      shell: mountpoint -q {{ base_data_path }} || echo "not mounted"
      register: mount_check
      changed_when: false

    - name: Wait for device to stabilize
      command: udevadm settle
      when: not raid_configured and mount_check.stdout == "not mounted"

    - name: Mount RAID array
      mount:
        path: "{{ base_data_path }}"
        src: "/dev/md0"
        fstype: xfs
        state: mounted
        opts: "defaults,noatime"
      when: not raid_configured and mount_check.stdout == "not mounted"

    - name: Verify mount
      shell: mountpoint -q {{ base_data_path }}
      register: mount_verify
      failed_when: mount_verify.rc != 0

    - name: Ensure base mount point permissions after mounting
      file:
        path: "{{ base_data_path }}"
        state: directory
        mode: "0755"
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"

    - name: Create arbitrum directory
      file:
        path: "{{ arbitrum_path }}"
        state: directory
        mode: "0755"
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"

    - name: Create Nitro logs directory
      file:
        path: "{{ nitro_logs_path }}"
        state: directory
        mode: "0755"
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"

    - name: Save RAID configuration
      shell: mdadm --detail --scan >> /etc/mdadm.conf
      args:
        creates: /etc/mdadm.conf

    - name: Update initramfs
      command: update-initramfs -u

    - name: Install Docker
      apt:
        name: docker.io
        state: present

    - name: Add ubuntu user to docker group
      user:
        name: "{{ ansible_user }}"
        groups: docker
        append: yes
      notify: restart docker

    - name: Ensure Docker service is running
      service:
        name: docker
        state: started
        enabled: yes

    - name: Fix ownership of existing files
      shell: |
        chown -R {{ ansible_user }}:{{ ansible_user }} {{ base_data_path }}
        find {{ arbitrum_path }} -type f -name "*.aria2" -delete 2>/dev/null || true
      ignore_errors: true

    - name: Get local manifest if exists
      stat:
        path: "{{ arbitrum_path }}/current.manifest"
      register: local_manifest

    - name: Read existing manifest content
      slurp:
        src: "{{ arbitrum_path }}/current.manifest"
      register: existing_manifest
      when: local_manifest.stat.exists
      ignore_errors: true

    - name: Get the latest snapshot directory
      uri:
        url: "https://snapshot.arbitrum.foundation/{{ chain_name }}/latest-{{ snapshot_type }}.txt"
        return_content: yes
      register: latest_snapshot

    - name: Set snapshot_dir fact
      set_fact:
        snapshot_dir: "{{ latest_snapshot.content | trim }}"

    - name: Fetch manifest file
      uri:
        url: "https://snapshot.arbitrum.foundation/{{ snapshot_dir }}.manifest.txt"
        return_content: yes
      register: manifest_file

    - name: Compare manifests
      set_fact:
        manifest_changed: "{{ not local_manifest.stat.exists or
          (local_manifest.stat.exists and
          manifest_file.content != (existing_manifest.content | b64decode)) }}"

    - name: Save current manifest
      copy:
        content: "{{ manifest_file.content }}"
        dest: "{{ arbitrum_path }}/current.manifest"
      when: manifest_changed

    - name: Initialize snapshot_parts list
      set_fact:
        snapshot_parts: []

    - name: Parse manifest lines into snapshot_parts
      set_fact:
        snapshot_parts: "{{ snapshot_parts + [{
          'checksum': item_split[0],
          'url': 'https://snapshot.arbitrum.foundation/' ~ chain_name ~ '/' ~ item_split[1],
          'filename': item_split[1] | regex_replace('^.*/', '')}] }}"
      loop: "{{ manifest_file.content.split('\n') }}"
      vars:
        item_split: "{{ item.split('  ') }}"
      when: item | length > 0

    - name: Install screen
      apt:
        name: screen
        state: present

    - name: Check if screen session already exists
      shell: screen -ls | grep aria2c_download || true
      register: screen_check
      changed_when: false

    - name: Kill existing screen session if it exists
      shell: screen -S aria2c_download -X quit
      when: screen_check.stdout != ""
      ignore_errors: true

    - name: Ensure no stale aria2c processes exist
      shell: |
        pkill -f aria2c || exit 0
        sleep 2  # Wait for processes to fully terminate
      when: manifest_changed
      become: true
      register: pkill_result
      failed_when: false
      changed_when: pkill_result.rc == 0

    - name: Ensure screen log directory exists
      file:
        path: "{{ arbitrum_path }}/logs"
        state: directory
        mode: "0755"
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"
      when: manifest_changed

    - name: Run aria2c in a detachable screen session
      when: manifest_changed
      shell: >
        cd {{ arbitrum_path }} &&
        screen -L -Logfile logs/aria2c_screen.log -dmS aria2c_download
        aria2c
        --console-log-level=warn
        --summary-interval=1
        --log="logs/aria2c.log"
        --log-level=warn
        -c -Z -x 16
        --auto-file-renaming=false
        {% for part in snapshot_parts %}"{{ part.url }}" {% endfor %}
      become: false

    - name: Verify screen session started correctly
      shell: >
        timeout 10 bash -c '
        while ! screen -ls | grep -q "aria2c_download"; do
          sleep 1;
        done'
      register: screen_check
      failed_when: screen_check.rc != 0
      when: manifest_changed
      become: false

    - name: Show instructions for viewing download progress
      when: manifest_changed
      debug:
        msg:
          - "aria2c is running in a screen session. To view the download progress:"
          - "1. SSH to the server: ssh {{ ansible_user }}@{{ ansible_host }}"
          - "2. Attach to the screen session: screen -r aria2c_download"
          - "3. To detach without killing the process: Ctrl+A followed by d"
          - "4. Alternatively, view the log file: tail -f {{ arbitrum_path }}/aria2c_download.log"

    - name: Wait for aria2c to complete
      when: manifest_changed
      shell: >
        while screen -list | grep -q "aria2c_download"; do
          if ! pgrep -f "aria2c.*{{ snapshot_parts[0].url }}" > /dev/null; then
            echo "aria2c process not found but screen session exists. Cleaning up.";
            screen -S aria2c_download -X quit;
            exit 1;
          fi;
          sleep 10;
        done
      args:
        executable: /bin/bash
      register: wait_result
      failed_when: wait_result.rc != 0
      become: false

    - name: Remove checksum validation status
      file:
        path: "{{ arbitrum_path }}/.checksums_validated"
        state: absent

    - name: Verify all snapshot parts were downloaded
      find:
        paths: "{{ arbitrum_path }}"
        patterns: "{{ item.filename }}"
        file_type: file
      register: downloaded_file
      loop: "{{ snapshot_parts }}"
      failed_when: downloaded_file.matched == 0

    - name: Get CPU thread count
      shell: nproc
      register: cpu_threads
      changed_when: false

    - name: Check if checksums were already validated
      stat:
        path: "{{ arbitrum_path }}/.checksums_validated"
      register: checksums_validated

    - name: Define checksums and filenames as lists with full paths
      set_fact:
        checksums_list: "{{ snapshot_parts | map(attribute='checksum') | list }}"
        filenames_list: "{{ snapshot_parts | map(attribute='filename') | map('regex_replace', '^(.*)$', arbitrum_path ~ '/\\1') | list }}"

    - name: Place the checksum verification script
      copy:
        src: verify_checksums.sh
        dest: /usr/local/bin/verify_checksums.sh
        mode: "0755"

    - name: Verify checksums with retry
      block:
        - name: Run the checksum script
          shell: >
            /usr/local/bin/verify_checksums.sh
            {{ checksums_list|length }}
            "{{ checksums_list | join(',') }}"
            "{{ filenames_list | join(',') }}"
          args:
            chdir: "{{ arbitrum_path }}"
          register: checksum_result
          failed_when: checksum_result.rc != 0 and checksum_result.rc != 2

        - name: Handle failed checksums
          block:
            - name: Read failed checksums file
              slurp:
                src: "{{ arbitrum_path }}/.checksum_failures.txt"
              register: failed_checksums_content

            - name: Extract failed filenames (improved)
              set_fact:
                failed_files: "{{ ((failed_checksums_content.content | b64decode) | default('')).split('\n') | select('regex', '.+') | list }}"
              when: failed_checksums_content is defined and failed_checksums_content.content is defined
              ignore_errors: true
              register: extraction_result

            - name: Set empty failed_files if extraction failed
              set_fact:
                failed_files: []
              when: failed_files is not defined or extraction_result is failed

            - name: Check if redownload screen session already exists
              shell: screen -ls | grep aria2c_redownload || true
              register: redownload_screen_check
              changed_when: false

            - name: Kill existing redownload screen session if it exists
              shell: screen -S aria2c_redownload -X quit
              when: redownload_screen_check.stdout != ""
              ignore_errors: true

            - name: Run aria2c redownload in a detachable screen session
              shell: >
                cd {{ arbitrum_path }} &&
                screen -L -Logfile logs/aria2c_redownload_screen.log -dmS aria2c_redownload
                aria2c
                --console-log-level=warn
                --summary-interval=1
                --log="logs/aria2c_redownload.log"
                --log-level=warn
                -c -Z -x 16
                --auto-file-renaming=false
                {% for file_entry in failed_files | default([]) %}
                {% set file_parts = file_entry.split('|') %}
                {% set filename = file_parts[0] | basename %}
                {% set file_url = snapshot_parts | selectattr('filename', 'equalto', filename) | map(attribute='url') | first %}
                "{{ file_url }}"
                {% endfor %}
              become: false
              when: failed_files is defined and failed_files | length > 0

            - name: Verify redownload screen session started correctly
              shell: >
                timeout 10 bash -c '
                while ! screen -ls | grep -q "aria2c_redownload"; do
                  sleep 1;
                done'
              register: redownload_screen_check
              failed_when: redownload_screen_check.rc != 0
              when: failed_files is defined and failed_files | length > 0
              become: false

            - name: Wait for aria2c redownload to complete
              shell: >
                while screen -list | grep -q "aria2c_redownload"; do
                  if ! pgrep -f "aria2c.*redownload" > /dev/null; then
                    echo "aria2c redownload process not found but screen session exists. Cleaning up.";
                    screen -S aria2c_redownload -X quit;
                    exit 1;
                  fi;
                  sleep 10;
                done
              args:
                executable: /bin/bash
              register: redownload_wait_result
              failed_when: redownload_wait_result.rc != 0
              become: false

            - name: Re-run checksum verification
              shell: >
                /usr/local/bin/verify_checksums.sh
                {{ checksums_list|length }}
                "{{ checksums_list | join(',') }}"
                "{{ filenames_list | join(',') }}"
              args:
                chdir: "{{ arbitrum_path }}"
              register: retry_result
              retries: 2
              delay: 5
              until: retry_result.rc == 0
              failed_when: false

            - name: Check if validation succeeded after retries
              stat:
                path: "{{ arbitrum_path }}/.checksums_validated"
              register: final_validation

            - name: Fail if validation still unsuccessful
              fail:
                msg: "Checksum validation failed after multiple attempts. Some files may be corrupted."
              when: not final_validation.stat.exists
          when: checksum_result.rc == 2
      when: not checksums_validated.stat.exists

    - name: Check for any content in Nitro directory
      find:
        paths: "{{ nitro_path }}"
        recurse: no
      register: nitro_contents

    - name: Skip extraction if Nitro directory is not empty
      when: nitro_contents.matched > 0
      debug:
        msg: "Nitro directory contains existing content. Skipping extraction."

    - name: Ensure Nitro directory exists
      file:
        path: "{{ nitro_path }}"
        state: directory
        mode: "0755"
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"

    - name: Concatenate and extract parts
      shell: |
        cat {% for part in snapshot_parts %}{{ part.filename }} {% endfor %} | tar --owner={{ ansible_user }} --group={{ ansible_user }} -xf - -C nitro
      args:
        chdir: "{{ arbitrum_path }}"
      when: nitro_contents.matched == 0

    - name: Check if container exists with correct configuration
      docker_container_info:
        name: arbitrum-nitro
      register: container_info

    - name: Set container status
      set_fact:
        container_needs_update: "{{ not container_info.exists or
          container_info.container.Config.Image != docker_image or
          container_info.container.State.Status != 'running' }}"

    - name: Start Arbitrum Nitro container
      when: container_needs_update
      docker_container:
        name: arbitrum-nitro
        image: "{{ docker_image }}"
        state: started
        restart_policy: unless-stopped
        user: "root"
        volumes:
          - "{{ arbitrum_path }}:/home/user/data:rw"
        ports:
          - "0.0.0.0:8547:8547"
          - "0.0.0.0:8548:8548"
        command: >
          --parent-chain.connection.url {{ parent_chain_url }}
          --parent-chain.blob-client.beacon-url {{ parent_chain_beacon_url }}
          --chain.id {{ chain_id }}
          --http.api=net,web3,eth,debug
          --http.corsdomain="*"
          --http.addr=0.0.0.0
          --http.vhosts="*"
          --persistent.chain=/home/user/data
          --file-logging.enable=true
          --file-logging.file=/home/user/data/logs/nitro.log
          --file-logging.max-size=100
          --file-logging.max-backups=20
          --file-logging.compress=true
          --file-logging.buf-size=512

    - name: Wait for Arbitrum Nitro rpc port to be open
      wait_for:
        host: 127.0.0.1
        port: 8547
        delay: 20
        timeout: 300

    - name: Send a test RPC request for current block number post
      uri:
        url: http://127.0.0.1:8547
        method: POST
        body: '{"jsonrpc":"2.0","method":"eth_blockNumber","params":[],"id":1}'
        body_format: json
        return_content: yes
      register: rpc_response
      retries: 9999999
      delay: 10
      until: rpc_response.status == 200

    - name: Ensure RPC response is valid
      assert:
        that:
          - rpc_response.status == 200
          - rpc_response.json.result is match("0x[0-9a-fA-F]+")
        fail_msg: "RPC response is invalid"
        success_msg: "RPC response is valid"
      register: rpc_response_valid
